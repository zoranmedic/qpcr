{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68db00e6",
   "metadata": {},
   "source": [
    "## ParCR evaluation\n",
    "\n",
    "This notebook goes through the steps needed to evaluate the encoder for Paragraph-level Citation Recommendation. Before running the notebook, you should have two JSON files ready: one with query embeddings and another with paper embeddings. These files are produced by `embed.py`, so make sure you run `embed.py` and have the output from the script ready when running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f1ddaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a802b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the JSON file containing data about papers that make up the pool\n",
    "PAPERS_POOL_PATH = 'data/test_pool_papers.json'\n",
    "\n",
    "# Path to the JSON file containing data about paragraphs that make up the queries that are being evaluated\n",
    "PARAGRAPH_LABELS_PATH = 'data/test_paragraphs.json'\n",
    "\n",
    "# Path to the JSON file containing query embeddings (output of `embed.py`)\n",
    "QUERY_EMBEDDINGS_PATH = 'queries_embeddings.json' Â # TODO replace with the output of `embed.py`\n",
    "\n",
    "# Path to the JSON file containing paper embeddings (output of `embed.py`)\n",
    "PAPER_EMBEDDINGS_PATH = 'papers_embeddings.json'  # TODO replace with the output of `embed.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcab014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = json.load(open(PAPERS_POOL_PATH))\n",
    "par_labels = json.load(open(PARAGRAPH_LABELS_PATH))\n",
    "query_embs_map = json.load(open(QUERY_EMBEDDINGS_PATH))\n",
    "paper_embs_map = json.load(open(PAPER_EMBEDDINGS_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40856135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2148 query embeddings and 94129 paper embeddings.\n"
     ]
    }
   ],
   "source": [
    "print(f'Loaded {len(query_embs_map)} query embeddings and {len(paper_embs_map)} paper embeddings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff4e9e",
   "metadata": {},
   "source": [
    "Next, we need to turn embeddings into numpy arrays and store query/paper ids into separate lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3a164ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids, query_embs = [], []\n",
    "for qid in query_embs_map:\n",
    "    query_ids.append(qid)\n",
    "    query_embs.append(np.array(query_embs_map[qid]).astype('float32'))\n",
    "\n",
    "paper_ids, paper_embs = [], []\n",
    "for pid in paper_embs_map:\n",
    "    paper_ids.append(pid)\n",
    "    paper_embs.append(np.array(paper_embs_map[pid]).astype('float32'))\n",
    "    \n",
    "paper_embs = np.array(paper_embs)\n",
    "query_embs = np.array(query_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ee977",
   "metadata": {},
   "source": [
    "We're now ready to create an index and index all the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6233c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index on GPU\n",
    "res = faiss.StandardGpuResources()\n",
    "index_flat = faiss.IndexFlatL2(paper_embs.shape[1])\n",
    "gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index_flat)\n",
    "\n",
    "# Index paper embeddings                          \n",
    "gpu_index_flat.add(paper_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc9f42c",
   "metadata": {},
   "source": [
    "Once we've created the index, we can perform search and retrieve nearest neighbours for the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65d7e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform search for all the query embeddings, retrieve top 1024 neighbours for each\n",
    "D, I = gpu_index_flat.search(query_embs, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6be55b8",
   "metadata": {},
   "source": [
    "Now we need a method that creates a list of nearest neighbour ids which we'll use to calculate metrics that will tell us how the encoder model performs - does it encode relevant articles close to the query or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c27fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_candidates_map(query_ids, D, I, keep_only_older_than_citing=False):\n",
    "    query_candidates = {}\n",
    "    for qid, scores, neighbours in zip(query_ids, D, I):\n",
    "        citing_id = qid.split(\"_\")[0]\n",
    "        neighbours_pids = [paper_ids[i] for i in neighbours]\n",
    "        if keep_only_older_than_citing:\n",
    "            neighbours_pids = [\n",
    "                i\n",
    "                for i in neighbours_pids\n",
    "                if pool[i][\"year\"] is None or pool[i][\"year\"] < pool[citing_id][\"year\"]\n",
    "            ]\n",
    "        query_candidates[qid] = [(i, s) for i, s in zip(neighbours_pids, scores) if i != citing_id][:1000]\n",
    "\n",
    "    for pid in query_candidates:\n",
    "        query_candidates[pid] = [(i[0], float(i[1])) for i in query_candidates[pid]]\n",
    "\n",
    "    return query_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abc9f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_candidates = get_query_candidates_map(query_ids, D, I, keep_only_older_than_citing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03681b02",
   "metadata": {},
   "source": [
    "Now we can calculate all the metrics we're interested in using the lists of nearest neighbours and articles actually cited in the query (data loaded from `PARAGRAPH_LABELS_PATH`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d46e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import recall, reciprocal_rank, average_precision, ndcg\n",
    "\n",
    "qid_data = []\n",
    "\n",
    "for qid in query_candidates:\n",
    "    cands = [i[0] for i in query_candidates[qid]]\n",
    "    if par_labels[qid]['citations']:\n",
    "        if isinstance(par_labels[qid]['citations'][0], str):\n",
    "            true = list(set(par_labels[qid]['citations']))\n",
    "        else:\n",
    "            true = list(set([i['ref_id'] for i in par_labels[qid]['citations']]))\n",
    "    qid_data.append({\n",
    "        'qid': qid,\n",
    "        'r@1': recall(true, cands, k=1),\n",
    "        'r@5': recall(true, cands, k=5),\n",
    "        'r@10': recall(true, cands, k=10),\n",
    "        'r@100': recall(true, cands, k=100),\n",
    "        'r-precision': recall(true, cands, k=len(true)),\n",
    "        'rec_rank': reciprocal_rank(true, cands, k=1000),\n",
    "        'avg_prec': average_precision(true, cands),\n",
    "        'ndcg': ndcg(true, cands)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016c48ce",
   "metadata": {},
   "source": [
    "Below we print the average results across all the queries for metrics of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a84f1e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@1 0.032\n",
      "r@5 0.11\n",
      "r@10 0.16\n",
      "r@100 0.41\n",
      "r-precision 0.084\n",
      "rec_rank 0.21\n",
      "avg_prec 0.093\n",
      "ndcg 0.25\n"
     ]
    }
   ],
   "source": [
    "for metric in ['r@1', 'r@5', 'r@10', 'r@100', 'r-precision', 'rec_rank', 'avg_prec', 'ndcg']:\n",
    "    print(metric, f\"{sum(i[metric] for i in qid_data) / len(qid_data):.2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qpcr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
